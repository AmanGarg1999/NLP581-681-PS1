{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from nltk.corpus import stopwords\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is definitely a must have if your state d...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>GENRE_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's a great place and I highly recommend it.</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>GENRE_B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I will see the doctors, take some blood tests ...</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>GOING_TO_PLACES</td>\n",
       "      <td>GENRE_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I can tell you about having my phone and elect...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>MONEY_ISSUE</td>\n",
       "      <td>GENRE_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Their steaks are 100% recommended!</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>GENRE_B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               TEXT Sentiment  \\\n",
       "0   0  This is definitely a must have if your state d...  POSITIVE   \n",
       "1   1      It's a great place and I highly recommend it.  POSITIVE   \n",
       "2   2  I will see the doctors, take some blood tests ...   NEUTRAL   \n",
       "3   3  I can tell you about having my phone and elect...  NEGATIVE   \n",
       "4   4                 Their steaks are 100% recommended!  POSITIVE   \n",
       "\n",
       "             Topic    Genre  \n",
       "0             NONE  GENRE_B  \n",
       "1             NONE  GENRE_B  \n",
       "2  GOING_TO_PLACES  GENRE_A  \n",
       "3      MONEY_ISSUE  GENRE_A  \n",
       "4             NONE  GENRE_B  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('PS1.1A_training_data.txt',sep=\"\\t\", names=[\"ID\",\"TEXT\",\"Sentiment\",\"Topic\",\"Genre\"])\n",
    "data.head(5)\n",
    "# data = pd.read_csv('train.csv')\n",
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global STOPWORDS = []\n",
    "def clean_TEXT(dataframe):\n",
    "    dataframe['TEXT'] = dataframe.TEXT.fillna('none')\n",
    "    #dataframe['Sentiment'] = dataframe.Sentiment.fillna('none')\n",
    "    #dataframe['Topic'] = dataframe.Topic.fillna('none')\n",
    "    dataframe['Genre'] = dataframe.Genre.fillna('none')\n",
    "    dataframe = dataframe[dataframe.Sentiment != 'NEUTRAL']\n",
    "    dataframe['TEXT'] = dataframe['TEXT'].str.lower()\n",
    "\n",
    "        STOPWORDS = stopwords\n",
    "    columns = list(dataframe.columns)\n",
    "    \n",
    "    for column in columns:\n",
    "        if column != 'ID':\n",
    "            dataframe[column] = dataframe[column].str.replace(r'\\W', ' ').str.replace(r'\\s$','')\n",
    "            \n",
    "    \n",
    "    for column in columns:\n",
    "        dataframe = remove_stop_words(dataframe,column)\n",
    "    \n",
    "    for column in columns:\n",
    "        dataframe = remove_special_characters(dataframe,column)\n",
    "    \n",
    "    #dataframe['Text_Genre'] = dataframe['TEXT'] + dataframe['Genre']\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "        \n",
    "def remove_stop_words(data_frame, column_name):\n",
    "    if column_name != 'ID':\n",
    "        data_frame[column_name] = data_frame[column_name].apply(lambda x: \" \".join([i for i in x.lower().split() if i not in STOPWORDS]))\n",
    "    return data_frame\n",
    "\n",
    "def remove_special_characters(data_frame, columns):\n",
    "    data_frame.columns = data_frame.columns.str.replace('[!,@,#,$,%,^,&,*,\\\",:,;,.]','')\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOPWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-984bd8261c18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_TEXT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-c2f0af9814e6>\u001b[0m in \u001b[0;36mclean_TEXT\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c2f0af9814e6>\u001b[0m in \u001b[0;36mremove_stop_words\u001b[0;34m(data_frame, column_name)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ID'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c2f0af9814e6>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ID'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c2f0af9814e6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ID'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOPWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "data = clean_TEXT(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['TEXT'], data['Sentiment'],test_size=0.25,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape), print(y_train.shape)\n",
    "print(X_test.shape), print(y_test.shape)\n",
    "#print(X_train.head(5))\n",
    "print(y_train.unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(object):\n",
    "    def __init__(self, n_gram=1, printing=False):\n",
    "        self.prior = defaultdict(int)\n",
    "        self.logprior = {}\n",
    "        self.bigdoc = defaultdict(list)\n",
    "        self.loglikelihoods = defaultdict(defaultdict)\n",
    "        self.V = []\n",
    "        self.n = n_gram\n",
    "\n",
    "    def compute_prior_and_bigdoc(self, training_set, training_labels):\n",
    "        for x, y in zip(training_set, training_labels):\n",
    "            all_words = x.split(\" \")\n",
    "            if self.n == 1:\n",
    "                grams = all_words\n",
    "            else:\n",
    "                grams = self.words_to_grams(all_words)\n",
    "\n",
    "            self.prior[y] += len(grams)\n",
    "            self.bigdoc[y].append(x)\n",
    "\n",
    "    def compute_vocabulary(self, documents):\n",
    "        vocabulary = set()\n",
    "\n",
    "        for doc in documents:\n",
    "            for word in doc.split(\" \"):\n",
    "                vocabulary.add(word.lower())\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    def count_word_in_classes(self):\n",
    "        counts = {}\n",
    "        for c in list(self.bigdoc.keys()):\n",
    "            docs = self.bigdoc[c]\n",
    "            counts[c] = defaultdict(int)\n",
    "            for doc in docs:\n",
    "                words = doc.split(\" \")\n",
    "                for word in words:\n",
    "                    counts[c][word] += 1\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def train(self, training_set, training_labels, alpha=1):\n",
    "        # Get number of documents\n",
    "        N_doc = len(training_set)\n",
    "\n",
    "        # Get vocabulary used in training set\n",
    "        self.V = self.compute_vocabulary(training_set)\n",
    "\n",
    "        # Create bigdoc\n",
    "        for x, y in zip(training_set, training_labels):\n",
    "            self.bigdoc[y].append(x)\n",
    "\n",
    "        # Get set of all classes\n",
    "        all_classes = set(training_labels)\n",
    "\n",
    "        # Compute a dictionary with all word counts for each class\n",
    "        self.word_count = self.count_word_in_classes()\n",
    "\n",
    "        # For each class\n",
    "        for c in all_classes:\n",
    "            # Get number of documents for that class\n",
    "            N_c = float(sum(training_labels == c))\n",
    "\n",
    "            # Compute logprior for class\n",
    "            self.logprior[c] = np.log(N_c / N_doc)\n",
    "\n",
    "            # Calculate the sum of counts of words in current class\n",
    "            total_count = 0\n",
    "            for word in self.V:\n",
    "                total_count += self.word_count[c][word]\n",
    "\n",
    "            # For every word, get the count and compute the log-likelihood for this class\n",
    "            for word in self.V:\n",
    "                count = self.word_count[c][word]\n",
    "                self.loglikelihoods[c][word] = np.log((count+ alpha) / (total_count+ alpha))\n",
    "\n",
    "    def predict(self, test_doc):\n",
    "        sums = {\n",
    "            0: 0,\n",
    "            1: 0,\n",
    "        }\n",
    "        for c in self.bigdoc.keys():\n",
    "            sums[c] = self.logprior[c]\n",
    "            words = test_doc.split(\" \")\n",
    "            for word in words:\n",
    "                if word in self.V:\n",
    "                    sums[c] += self.loglikelihoods[c][word]\n",
    "\n",
    "        return sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(validation_set,validation_labels,trained_classifier):\n",
    "    correct_predictions = 0\n",
    "    predictions_list = []\n",
    "    prediction = -1\n",
    "    \n",
    "    for dataset,label in zip(validation_set, validation_labels):\n",
    "        #print(label)\n",
    "        probabilities = trained_classifier.predict(dataset)\n",
    "        if probabilities[0] >= probabilities[1]:\n",
    "            prediction = 'negative'\n",
    "        elif  probabilities[0] < probabilities[1]:\n",
    "            prediction = 'positive'\n",
    "        if prediction == label:\n",
    "            correct_predictions += 1\n",
    "            predictions_list.append(\"+\")\n",
    "        else:\n",
    "            predictions_list.append(\"-\")\n",
    "    pp.pprint(\"Predicted correctly {} out of {} ({}%)\".format(correct_predictions,len(validation_labels),round(correct_predictions/len(validation_labels)*100,2)))\n",
    "    return predictions_list, round(correct_predictions/len(validation_labels)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBC = NaiveBayesClassifier()\n",
    "NBC.train(data['TEXT'], data['Sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = clean_TEXT(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9991a1798a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After removing the STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Predicted correctly 1013 out of 2015 (50.27%)'\n",
      "['-', '-', '-', '-', '+', '+', '-', '+', '+', '-', '-', '-', '+', '-', '-', '+', '+', '+', '+', '-', '+', '+', '+', '+', '-', '+', '+', '+', '+', '-', '-', '-', '-', '+', '+', '-', '-', '+', '+', '-', '+', '+', '+', '-', '+', '+', '-', '-', '+', '+', '+', '+', '+', '-', '-', '-', '-', '+', '+', '+', '-', '-', '-', '+', '-', '+', '-', '-', '-', '+', '+', '-', '-', '+', '-', '-', '+', '+', '-', '-', '-', '-', '-', '+', '+', '-', '+', '-', '+', '-', '+', '+', '-', '-', '+', '-', '-', '-', '+', '+', '+', '-', '-', '+', '+', '+', '-', '+', '-', '-', '-', '+', '+', '-', '+', '+', '-', '-', '+', '-', '+', '-', '+', '+', '-', '-', '+', '+', '+', '-', '+', '-', '+', '-', '-', '+', '-', '+', '-', '-', '-', '+', '+', '+', '-', '-', '-', '+', '+', '+', '+', '+', '-', '-', '+', '+', '+', '-', '-', '-', '-', '+', '-', '+', '+', '-', '+', '-', '-', '-', '-', '-', '-', '-', '+', '-', '+', '-', '-', '+', '+', '-', '+', '-', '-', '-', '-', '+', '-', '-', '+', '-', '-', '-', '+', '+', '-', '+', '-', '-', '-', '+', '+', '-', '-', '+', '-', '-', '-', '-', '+', '-', '+', '-', '+', '-', '+', '-', '+', '-', '-', '-', '+', '+', '+', '-', '+', '-', '-', '+', '+', '+', '+', '-', '-', '+', '-', '-', '+', '-', '+', '-', '+', '+', '+', '-', '+', '+', '-', '+', '-', '+', '-', '-', '+', '-', '-', '+', '-', '+', '+', '+', '+', '+', '+', '-', '+', '+', '-', '-', '-', '+', '+', '-', '+', '-', '+', '-', '+', '-', '-', '-', '+', '+', '+', '-', '+', '-', '+', '-', '+', '+', '+', '-', '+', '-', '+', '-', '-', '-', '-', '+', '+', '+', '-', '+', '-', '-', '+', '+', '+', '+', '+', '+', '-', '-', '-', '+', '+', '+', '+', '+', '-', '-', '+', '-', '+', '-', '+', '-', '-', '+', '+', '+', '-', '-', '-', '-', '-', '+', '-', '+', '-', '-', '+', '+', '-', '-', '+', '+', '-', '+', '-', '+', '-', '-', '-', '-', '-', '-', '-', '+', '-', '+', '+', '+', '+', '+', '-', '+', '+', '-', '+', '+', '-', '-', '-', '+', '+', '+', '-', '+', '+', '+', '-', '+', '-', '+', '+', '+', '-', '+', '+', '+', '+', '-', '-', '+', '+', '+', '+', '-', '-', '+', '-', '+', '+', '-', '-', '+', '+', '-', '-', '-', '-', '+', '-', '-', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '+', '-', '+', '-', '-', '-', '-', '-', '+', '-', '-', '-', '+', '+', '-', '-', '+', '-', '-', '+', '+', '+', '+', '-', '+', '-', '-', '-', '+', '+', '-', '+', '-', '-', '+', '+', '+', '-', '+', '+', '+', '+', '-', '+', '-', '+', '+', '-', '+', '-', '-', '-', '+', '+', '+', '-', '-', '-', '-', '-', '-', '-', '+', '-', '+', '+', '+', '-', '-', '-', '+', '-', '-', '+', '+', '-', '+', '-', '-', '-', '-', '-', '+', '-', '-', '-', '+', '-', '-', '-', '+', '+', '-', '+', '+', '-', '+', '+', '-', '+', '+', '+', '+', '+', '-', '-', '-', '+', '-', '-', '+', '+', '-', '+', '-', '-', '-', '-', '-', '+', '+', '-', '+', '-', '-', '-', '+', '-', '-', '+', '+', '+', '-', '+', '-', '+', '-', '+', '-', '+', '+', '-', '+', '-', '-', '-', '-', '+', '+', '+', '+', '-', '-', '-', '-', '-', '+', '+', '-', '-', '+', '-', '+', '+', '+', '+', '-', '+', '+', '+', '-', '-', '-', '-', '+', '+', '-', '+', '+', '-', '+', '+', '-', '+', '+', '+', '+', '-', '+', '+', '+', '-', '-', '+', '-', '-', '-', '-', '+', '-', '+', '-', '-', '-', '-', '-', '+', '+', '-', '+', '+', '-', '+', '-', '-', '+', '+', '+', '+', '+', '-', '-', '+', '-', '-', '-', '-', '-', '-', '+', '-', '-', '+', '+', '-', '+', '-', '+', '+', '-', '+', '-', '-', '-', '-', '+', '+', '+', '+', '+', '-', '-', '+', '-', '+', '-', '+', '+', '-', '+', '-', '+', '-', '-', '-', '-', '-', '+', '-', '+', '+', '+', '-', '+', '-', '-', '+', '-', '-', '+', '-', '-', '+', '+', '+', '+', '-', '+', '+', '-', '-', '+', '-', '-', '+', '+', '-', '+', '+', '-', '+', '+', '-', '+', '+', '-', '+', '-', '+', '-', '-', '-', '-', '+', '+', '-', '+', '+', '-', '+', '-', '-', '-', '+', '-', '+', '-', '+', '+', '+', '+', '-', '+', '-', '+', '+', '+', '+', '-', '-', '-', '+', '-', '+', '+', '-', '+', '+', '+', '+', '+', '-', '+', '+', '-', '-', '-', '+', '-', '-', '+', '-', '+', '+', '+', '+', '+', '-', '+', '-', '+', '+', '-', '-', '+', '+', '+', '-', '-', '+', '+', '-', '+', '-', '-', '+', '+', '-', '+', '-', '+', '+', '-', '-', '+', '+', '+', '+', '+', '-', '+', '+', '-', '-', '-', '+', '-', '-', '+', '-', '+', '+', '-', '+', '-', '+', '+', '+', '+', '-', '-', '+', '+', '+', '-', '-', '+', '-', '-', '+', '+', '+', '+', '+', '-', '+', '+', '-', '+', '-', '-', '+', '-', '+', '-', '+', '-', '+', '-', '+', '-', '-', '-', '+', '+', '-', '+', '+', '-', '+', '-', '+', '-', '-', '+', '+', '+', '-', '+', '+', '-', '-', '+', '+', '-', '+', '+', '-', '-', '+', '+', '-', '+', '+', '+', '-', '+', '-', '-', '-', '-', '+', '-', '+', '+', '+', '-', '-', '+', '-', '+', '-', '-', '-', '+', '-', '+', '+', '-', '+', '-', '-', '-', '+', '+', '-', '-', '+', '-', '+', '-', '+', '-', '-', '+', '-', '+', '+', '+', '-', '-', '+', '-', '-', '+', '-', '+', '-', '-', '+', '-', '-', '-', '-', '-', '-', '-', '-', '+', '+', '+', '-', '+', '+', '+', '-', '+', '+', '+', '+', '+', '+', '-', '+', '-', '-', '+', '+', '-', '+', '+', '-', '+', '-', '-', '-', '+', '-', '+', '+', '-', '+', '+', '-', '+', '-', '-', '-', '-', '+', '-', '-', '+', '+', '+', '-', '+', '-', '+', '-', '+', '+', '+', '-', '+', '-', '+', '-', '+', '-', '+', '-', '-', '+', '+', '+', '+', '-', '-', '-', '+', '-', '-', '-', '-', '+', '+', '-', '-', '-', '-', '+', '+', '+', '+', '+', '-', '+', '-', '+', '-', '+', '+', '+', '-', '+', '-', '-', '-', '+', '+', '-', '-', '+', '-', '+', '+', '-', '-', '+', '+', '-', '+', '-', '+', '-', '+', '-', '-', '+', '-', '-', '+', '-', '+', '+', '+', '+', '+', '-', '+', '-', '-', '-', '+', '+', '+', '+', '+', '-', '-', '-', '+', '-', '+', '+', '+', '-', '+', '-', '-', '+', '-', '-', '+', '+', '-', '+', '+', '+', '+', '-', '+', '+', '+', '+', '-', '+', '-', '+', '+', '+', '-', '+', '-', '-', '+', '-', '+', '+', '+', '-', '-', '+', '+', '+', '+', '+', '+', '+', '+', '+', '-', '+', '-', '-', '-', '-', '+', '-', '-', '+', '+', '+', '+', '+', '+', '-', '-', '+', '+', '-', '+', '+', '+', '+', '-', '+', '+', '+', '+', '-', '+', '-', '-', '+', '-', '-', '+', '+', '-', '-', '+', '+', '+', '+', '-', '+', '-', '-', '+', '-', '+', '-', '-', '+', '+', '-', '-', '-', '+', '+', '-', '-', '-', '+', '-', '+', '+', '-', '-', '+', '-', '-', '+', '+', '+', '-', '-', '-', '-', '+', '-', '-', '-', '+', '+', '+', '-', '-', '-', '+', '+', '+', '+', '-', '-', '+', '-', '-', '-', '+', '-', '-', '-', '-', '+', '-', '-', '-', '+', '+', '-', '-', '+', '+', '+', '+', '-', '-', '+', '-', '+', '+', '-', '-', '-', '-', '-', '+', '-', '+', '-', '+', '-', '-', '+', '-', '+', '-', '+', '-', '+', '-', '+', '-', '+', '+', '+', '-', '-', '-', '+', '-', '+', '-', '-', '-', '+', '+', '-', '+', '-', '+', '-', '+', '-', '+', '-', '+', '-', '-', '-', '-', '-', '+', '+', '-', '-', '+', '-', '-', '+', '+', '-', '+', '-', '-', '-', '+', '-', '+', '+', '-', '+', '+', '-', '-', '+', '-', '+', '-', '+', '+', '+', '+', '+', '+', '-', '-', '-', '+', '+', '+', '-', '+', '+', '+', '-', '+', '+', '+', '+', '-', '+', '-', '-', '-', '-', '+', '+', '-', '-', '+', '-', '+', '-', '-', '+', '-', '+', '-', '+', '-', '-', '-', '+', '+', '+', '+', '-', '-', '+', '-', '-', '+', '+', '+', '+', '+', '+', '+', '-', '-', '+', '-', '+', '-', '-', '+', '-', '+', '-', '+', '+', '-', '+', '+', '+', '-', '-', '+', '+', '+', '+', '+', '+', '+', '-', '+', '+', '+', '+', '-', '-', '-', '+', '-', '+', '+', '-', '-', '-', '-', '+', '-', '+', '-', '+', '-', '+', '+', '+', '+', '+', '-', '+', '-', '+', '-', '-', '+', '+', '+', '-', '+', '+', '+', '+', '-', '+', '+', '-', '-', '+', '-', '-', '-', '+', '-', '-', '-', '+', '+', '+', '-', '+', '+', '+', '-', '+', '-', '-', '-', '-', '-', '+', '+', '-', '+', '-', '+', '+', '-', '-', '+', '-', '-', '-', '-', '-', '+', '-', '+', '+', '-', '-', '-', '+', '-', '+', '-', '-', '+', '+', '-', '+', '-', '+', '-', '+', '+', '-', '+', '+', '+', '+', '-', '+', '+', '+', '+', '-', '+', '+', '-', '+', '+', '-', '-', '-', '+', '-', '-', '-', '+', '+', '-', '-', '-', '+', '-', '+', '-', '+', '+', '-', '-', '-', '-', '-', '+', '+', '-', '-', '-', '+', '-', '-', '-', '+', '-', '-', '-', '-', '+', '+', '-', '+', '-', '+', '-', '+', '+', '+', '+', '-', '+', '+', '-', '+', '+', '-', '-', '-', '-', '-', '-', '+', '-', '+', '-', '+', '-', '-', '+', '-', '+', '+', '-', '-', '+', '+', '-', '-', '-', '-', '-', '+', '-', '-', '+', '+', '+', '-', '+', '-', '+', '+', '-', '-', '+', '+', '+', '+', '-', '-', '-', '+', '-', '-', '-', '-', '-', '+', '+', '-', '-', '+', '-', '-', '+', '-', '-', '-', '-', '+', '-', '-', '-', '+', '-', '+', '+', '-', '-', '-', '+', '+', '-', '-', '+', '-', '+', '+', '-', '-', '+', '-', '-', '+', '-', '+', '-', '-', '+', '-', '-', '-', '+', '-', '+', '+', '-', '-', '+', '+', '-', '+', '+', '-', '-', '-', '-', '-', '-', '-', '+', '-', '-', '+', '-', '+', '+', '-', '+', '+', '+', '+', '+', '+', '+', '-', '+', '-', '-', '+', '+', '+', '-', '+', '+', '-', '+', '-', '+', '-', '-', '-', '+', '-', '+', '+', '+', '+', '+', '-', '+', '-', '-', '-', '-', '+', '-', '+', '-', '+', '-', '+', '-', '+', '-', '+', '+', '-', '+', '+', '+', '-', '-', '-', '+', '+', '+', '-', '+', '+', '-', '-', '+', '-', '-', '-', '+', '-', '-', '+', '-', '-', '-', '-', '+', '-', '-', '-', '-', '+', '-', '-', '+', '+', '+', '+', '-', '+', '+', '-', '-', '+', '-', '+', '+', '-', '-', '-', '-', '-', '+', '+', '+', '+', '+', '+', '+', '-', '+', '+', '+', '-', '-', '+', '+', '-', '-', '+', '+', '+', '+', '+', '-', '+', '-', '+', '-', '-', '-', '+', '-', '+', '+', '-', '-', '+', '+', '-', '-', '-', '+', '-', '+', '-', '+', '-', '-', '-', '-', '+', '-', '-', '-', '-', '+', '-', '-', '+', '+', '-', '+', '-', '+', '+', '-', '+', '+', '+', '-', '+', '-', '-', '-', '+', '+', '-', '-', '+', '-', '+', '+', '-', '+', '+', '+', '+', '-', '+', '-', '+', '-', '+', '+', '-', '+', '+', '-', '-', '+', '-', '+', '-', '+', '+', '-', '-', '-', '+', '+', '-', '+', '+', '+', '+', '+', '-', '+', '-', '-', '-', '+', '+', '-', '+', '+', '+', '+', '-', '-', '+', '+', '+', '-', '+', '+', '-', '+', '-', '-', '+', '+', '+', '-', '-', '-', '-', '+', '+', '-', '-', '+', '-', '+', '+', '+'] 50\n"
     ]
    }
   ],
   "source": [
    "result, acc = evaluate_predictions(test_df['TEXT'], test_df['Sentiment'],NBC)\n",
    "print(result, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After not removing the STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, acc = evaluate_predictions(test_df['TEXT'], test_df['Sentiment'],NBC)\n",
    "print(result, acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
